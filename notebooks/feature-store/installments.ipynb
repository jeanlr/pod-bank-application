{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73d53d8-d659-4b15-bd2e-bf2d840e6a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 17:59:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import to_date, lit, date_format\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Installments\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50c36cd-02d8-438f-bb98-b533bfe40d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas: 13605401\n",
      "Quantidade de variaveis (colunas): 8\n",
      "+----------+----------+----------------------+---------------------+---------------+------------------+--------------+-----------+\n",
      "|SK_ID_PREV|SK_ID_CURR|NUM_INSTALMENT_VERSION|NUM_INSTALMENT_NUMBER|DAYS_INSTALMENT|DAYS_ENTRY_PAYMENT|AMT_INSTALMENT|AMT_PAYMENT|\n",
      "+----------+----------+----------------------+---------------------+---------------+------------------+--------------+-----------+\n",
      "|1054186   |161674    |1.0                   |6                    |-1180.0        |-1187.0           |6948.36       |6948.36    |\n",
      "|1330831   |151639    |0.0                   |34                   |-2156.0        |-2156.0           |1716.525      |1716.525   |\n",
      "|2085231   |193053    |2.0                   |1                    |-63.0          |-63.0             |25425.0       |25425.0    |\n",
      "|2452527   |199697    |1.0                   |3                    |-2418.0        |-2426.0           |24350.13      |24350.13   |\n",
      "|2714724   |167756    |1.0                   |2                    |-1383.0        |-1366.0           |2165.04       |2160.585   |\n",
      "+----------+----------+----------------------+---------------------+---------------+------------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "installments = spark.read.csv(\"/data/raw/installments_payments.csv\",\n",
    "                               header=True,\n",
    "                               inferSchema=True)\n",
    "\n",
    "installments.createOrReplaceTempView(\"installments\")\n",
    "\n",
    "# Contagem de linhas e colunas\n",
    "num_rows = installments.count()\n",
    "num_columns = len(installments.columns)\n",
    "\n",
    "print(f'Quantidade de linhas: {num_rows}')\n",
    "print(f'Quantidade de variaveis (colunas): {num_columns}')\n",
    "\n",
    "installments.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3028d-f5f5-4756-b6dd-219cb03da65b",
   "metadata": {},
   "source": [
    "## Criando flags de janela temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68c1029-126b-483a-94d8-e98d3180c923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SK_ID_PREV: int, SK_ID_CURR: int, NUM_INSTALMENT_VERSION: double, NUM_INSTALMENT_NUMBER: int, DAYS_INSTALMENT: double, DAYS_ENTRY_PAYMENT: double, AMT_INSTALMENT: double, AMT_PAYMENT: double, U3M: int, U6M: int, U12M: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_temp_01 = spark.sql('''\n",
    "  SELECT\n",
    "    *,\n",
    "    CASE WHEN DAYS_INSTALMENT >= -90 THEN 1 ELSE 0 END AS U3M,\n",
    "    CASE WHEN DAYS_INSTALMENT >= -180 THEN 1 ELSE 0 END AS U6M,    \n",
    "    CASE WHEN DAYS_INSTALMENT >= -360 THEN 1 ELSE 0 END AS U12M\n",
    "  FROM installments\n",
    "  ORDER BY SK_ID_PREV\n",
    "''')\n",
    "\n",
    "df_temp_01.createOrReplaceTempView('df_temp_01')\n",
    "display(df_temp_01.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e762c-de40-425f-b58c-7ca40271bc1c",
   "metadata": {},
   "source": [
    "## Criando variáveis de primeira camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63524fc-853f-46b1-befc-0c6800687b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade Total de Variáveis Criadas: 24\n",
      "Nomes das Variáveis Criadas: ['QT_MAX_DAYS_INSTALMENT_U3M_INSTALLMENTS', 'QT_MIN_DAYS_INSTALMENT_U3M_INSTALLMENTS', 'QT_MAX_DAYS_INSTALMENT_U6M_INSTALLMENTS', 'QT_MIN_DAYS_INSTALMENT_U6M_INSTALLMENTS', 'QT_MAX_DAYS_INSTALMENT_U12M_INSTALLMENTS', 'QT_MIN_DAYS_INSTALMENT_U12M_INSTALLMENTS', 'QT_MAX_DAYS_ENTRY_PAYMENT_U3M_INSTALLMENTS', 'QT_MIN_DAYS_ENTRY_PAYMENT_U3M_INSTALLMENTS', 'QT_MAX_DAYS_ENTRY_PAYMENT_U6M_INSTALLMENTS', 'QT_MIN_DAYS_ENTRY_PAYMENT_U6M_INSTALLMENTS', 'QT_MAX_DAYS_ENTRY_PAYMENT_U12M_INSTALLMENTS', 'QT_MIN_DAYS_ENTRY_PAYMENT_U12M_INSTALLMENTS', 'VL_TOT_AMT_INSTALMENT_U3M_INSTALLMENTS', 'VL_MED_AMT_INSTALMENT_U3M_INSTALLMENTS', 'VL_TOT_AMT_INSTALMENT_U6M_INSTALLMENTS', 'VL_MED_AMT_INSTALMENT_U6M_INSTALLMENTS', 'VL_TOT_AMT_INSTALMENT_U12M_INSTALLMENTS', 'VL_MED_AMT_INSTALMENT_U12M_INSTALLMENTS', 'VL_TOT_AMT_PAYMENT_U3M_INSTALLMENTS', 'VL_MED_AMT_PAYMENT_U3M_INSTALLMENTS', 'VL_TOT_AMT_PAYMENT_U6M_INSTALLMENTS', 'VL_MED_AMT_PAYMENT_U6M_INSTALLMENTS', 'VL_TOT_AMT_PAYMENT_U12M_INSTALLMENTS', 'VL_MED_AMT_PAYMENT_U12M_INSTALLMENTS']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============>                                            (2 + 6) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas do DataFrame: 997752\n",
      "Quantidade de colunas do DataFrame: 25\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[SK_ID_PREV: int, QT_MAX_DAYS_INSTALMENT_U3M_INSTALLMENTS: double, QT_MIN_DAYS_INSTALMENT_U3M_INSTALLMENTS: double, QT_MAX_DAYS_INSTALMENT_U6M_INSTALLMENTS: double, QT_MIN_DAYS_INSTALMENT_U6M_INSTALLMENTS: double, QT_MAX_DAYS_INSTALMENT_U12M_INSTALLMENTS: double, QT_MIN_DAYS_INSTALMENT_U12M_INSTALLMENTS: double, QT_MAX_DAYS_ENTRY_PAYMENT_U3M_INSTALLMENTS: double, QT_MIN_DAYS_ENTRY_PAYMENT_U3M_INSTALLMENTS: double, QT_MAX_DAYS_ENTRY_PAYMENT_U6M_INSTALLMENTS: double, QT_MIN_DAYS_ENTRY_PAYMENT_U6M_INSTALLMENTS: double, QT_MAX_DAYS_ENTRY_PAYMENT_U12M_INSTALLMENTS: double, QT_MIN_DAYS_ENTRY_PAYMENT_U12M_INSTALLMENTS: double, VL_TOT_AMT_INSTALMENT_U3M_INSTALLMENTS: double, VL_MED_AMT_INSTALMENT_U3M_INSTALLMENTS: double, VL_TOT_AMT_INSTALMENT_U6M_INSTALLMENTS: double, VL_MED_AMT_INSTALMENT_U6M_INSTALLMENTS: double, VL_TOT_AMT_INSTALMENT_U12M_INSTALLMENTS: double, VL_MED_AMT_INSTALMENT_U12M_INSTALLMENTS: double, VL_TOT_AMT_PAYMENT_U3M_INSTALLMENTS: double, VL_MED_AMT_PAYMENT_U3M_INSTALLMENTS: double, VL_TOT_AMT_PAYMENT_U6M_INSTALLMENTS: double, VL_MED_AMT_PAYMENT_U6M_INSTALLMENTS: double, VL_TOT_AMT_PAYMENT_U12M_INSTALLMENTS: double, VL_MED_AMT_PAYMENT_U12M_INSTALLMENTS: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round, sum, avg, max, min, when, countDistinct, count, date_format, current_date\n",
    "# Definindo as colunas para a agregação.\n",
    "colunas_agregacao_total = df_temp_01.columns\n",
    "colunas_agregacao_total.remove('SK_ID_CURR')\n",
    "colunas_agregacao_total.remove('SK_ID_PREV')\n",
    "colunas_agregacao_total.remove('NUM_INSTALMENT_VERSION')\n",
    "colunas_agregacao_total.remove('NUM_INSTALMENT_NUMBER')\n",
    "\n",
    "# Defindo a lista de colunas de flags.\n",
    "colunas_flags = ['U3M', 'U6M', 'U12M']\n",
    "\n",
    "# Criando uma lista vazia.\n",
    "expressoes_agregacao = []\n",
    "\n",
    "# Iterando sobre as colunas e criando as variáveis explicativas com as agregações.\n",
    "for coluna in colunas_agregacao_total:\n",
    "  # Verifica se a coluna atual não é uma coluna de flag.\n",
    "  if not any(flag in coluna for flag in colunas_flags):\n",
    "    for flag in colunas_flags:\n",
    "      if 'DAYS' in coluna:\n",
    "        expressoes_agregacao.append(round(max(when(col(flag) == 1, col(coluna))), 2).alias(f'QT_MAX_{coluna.upper()}_{flag.upper()}_INSTALLMENTS'))\n",
    "        expressoes_agregacao.append(round(min(when(col(flag) == 1, col(coluna))), 2).alias(f'QT_MIN_{coluna.upper()}_{flag.upper()}_INSTALLMENTS'))\n",
    "      else:\n",
    "        expressoes_agregacao.append(round(sum(when(col(flag) == 1, col(coluna))), 2).alias(f'VL_TOT_{coluna.upper()}_{flag.upper()}_INSTALLMENTS'))\n",
    "        expressoes_agregacao.append(round(avg(when(col(flag) == 1, col(coluna))), 2).alias(f'VL_MED_{coluna.upper()}_{flag.upper()}_INSTALLMENTS'))  \n",
    "\n",
    "\n",
    "# Criando uma tupla com as variáveis criadas.\n",
    "expressoes_agregacao = tuple(expressoes_agregacao)\n",
    "\n",
    "# Aplicando as expressões de agregação.\n",
    "df_temp_02 = df_temp_01.groupBy('SK_ID_PREV').agg(*expressoes_agregacao).orderBy('SK_ID_PREV')\n",
    "\n",
    "\n",
    "# Quantidade e nome das variáveis criadas.\n",
    "nomes_cols = df_temp_02.columns\n",
    "nomes_cols_novas = nomes_cols[1:]\n",
    "print('Quantidade Total de Variáveis Criadas:', len(df_temp_02.columns) - 1)\n",
    "print('Nomes das Variáveis Criadas:', nomes_cols_novas)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Quantidade de linhas do DataFrame.\n",
    "num_rows_df = df_temp_02.count()\n",
    "\n",
    "# Quantidade de colunas do DataFrame.\n",
    "num_columns_df = len(df_temp_02.columns)\n",
    "\n",
    "# Imprimir o resultado de número de linhas e colunas.\n",
    "print(f'Quantidade de linhas do DataFrame: {num_rows_df}')\n",
    "print(f'Quantidade de colunas do DataFrame: {num_columns_df}')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "# Mostrando o novo DataFrame com as variáveis criadas.\n",
    "display(df_temp_02.limit(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c99c93-202b-4f17-a4c5-a177e280b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 18:00:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_temp_02.write.mode(\"overwrite\").parquet('/data/books/installments')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6933bdec-7d62-41bc-9dd7-6a5b6b4e8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
