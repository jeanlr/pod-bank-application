{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effbb586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 08:58:46 WARN Utils: Your hostname, DESKTOP-A7MMD62 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/22 08:58:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/22 08:58:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import to_date, lit, date_format\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5568dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application_test= spark.read.csv(\"../../data/raw/application_test.csv\",\n",
    "                               header=True,\n",
    "                               inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9af29",
   "metadata": {},
   "source": [
    "## Verificando quantidade de linhas e colunas no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a197f6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 08:58:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas do DataFrame: 92254\n",
      "Quantidade de colunas do DataFrame: 171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====>                                                    (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|distinct_id_pos|\n",
      "+---------------+\n",
      "|          92254|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_application_test.createOrReplaceTempView(\"dados\")\n",
    "# Verificando a quantidade de linhas e colunas do DataFrame.\n",
    "\n",
    "# Quantidade de linhas.\n",
    "qtt_rows = df_application_test.count()\n",
    "\n",
    "# Quantidade de colunas.\n",
    "qtt_columns = len(df_application_test.columns)\n",
    "\n",
    "# Quantidade de IDs únicos.\n",
    "distinct_id_pos = spark.sql('''SELECT COUNT(DISTINCT `SK_ID_CURR`) as distinct_id_pos FROM dados ''')\n",
    "distinct_id_pos.createOrReplaceTempView(\"distinct_id_pos\")\n",
    "\n",
    "# Imprimir o resultado.\n",
    "print(f'Quantidade de linhas do DataFrame: {qtt_rows}')\n",
    "print(f'Quantidade de colunas do DataFrame: {qtt_columns}')\n",
    "distinct_id_pos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8acd7",
   "metadata": {},
   "source": [
    "## Lendo os dados de Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8cb02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bureau= spark.read.parquet(\"../../data/books/bureau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94144ad8",
   "metadata": {},
   "source": [
    "## Join da base de Application_Train com Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10db2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test_full = df_application_test.join(df_bureau, \"SK_ID_CURR\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52cd3d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Colunas: 325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total de Colunas: \" + str(len(app_test_full.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895379c3",
   "metadata": {},
   "source": [
    "## Lendo os dados de Previous Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fa541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_previous_app = spark.read.parquet(\"../../data/books/previous-app-fs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1ee6a",
   "metadata": {},
   "source": [
    "## Join da base de Application_Train_Full com Previous Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb5fce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test_full = app_test_full.join(df_previous_app, \"SK_ID_CURR\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3cbad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Colunas: 551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas: 92254\n"
     ]
    }
   ],
   "source": [
    "print(\"Total de Colunas: \" + str(len(app_test_full.columns)))\n",
    "print(\"Total de linhas: \" + str(app_test_full.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e1979",
   "metadata": {},
   "source": [
    "## Salvando ABT em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c9077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reparticionar para um único arquivo\n",
    "abt_test = app_test_full.repartition(1)\n",
    "abt_test.write.mode(\"overwrite\").parquet('../../data/abt/abt_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19f37d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
